"""
Script t·∫°o Hybrid Embeddings (Content + Collaborative)
Thay th·∫ø TF-IDF b·∫±ng embeddings n√¢ng cao:
1. Content Embeddings: Sentence-BERT
2. Collaborative Embeddings: SVD (Matrix Factorization)
3. Hybrid Embeddings: K·∫øt h·ª£p c·∫£ 2
"""

import pandas as pd
import numpy as np
import os
import pickle
from pathlib import Path
from sentence_transformers import SentenceTransformer
from surprise import SVD, Dataset, Reader
from sklearn.preprocessing import normalize
import warnings
warnings.filterwarnings('ignore')

# ƒê∆∞·ªùng d·∫´n - relative t·ª´ project root
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = str(BASE_DIR / "data_cleaned")
OUTPUT_DIR = str(BASE_DIR / "embeddings")
MODELS_DIR = str(BASE_DIR / "models")

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
for dir_path in [OUTPUT_DIR, MODELS_DIR]:
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

def load_cleaned_data():
    """Load d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch"""
    print("=" * 80)
    print("LOADING CLEANED DATA...")
    print("=" * 80)
    
    movies = pd.read_csv(os.path.join(DATA_DIR, "movies_cleaned.csv"))
    ratings = pd.read_csv(os.path.join(DATA_DIR, "ratings_cleaned.csv"))
    
    print(f"‚úÖ Loaded {len(movies)} movies")
    print(f"‚úÖ Loaded {len(ratings)} ratings")
    
    return movies, ratings

def create_content_embeddings(movies):
    """
    T·∫°o Content Embeddings b·∫±ng Sentence-BERT
    Thay th·∫ø TF-IDF b·∫±ng embeddings n√¢ng cao
    """
    print("\n" + "=" * 80)
    print("CREATING CONTENT EMBEDDINGS (Sentence-BERT)")
    print("=" * 80)
    
    print("\n1. Loading Sentence-BERT model...")
    # Model nh·ªè, nhanh, ƒë·ªß t·ªët cho recommendation
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print("   ‚úÖ Model loaded: all-MiniLM-L6-v2 (384 dimensions)")
    
    print("\n2. Creating embeddings from content_text...")
    content_texts = movies['content_text'].tolist()
    
    print(f"   - Processing {len(content_texts)} movies...")
    print("   - This may take a few minutes...")
    
    # T·∫°o embeddings (batch processing t·ª± ƒë·ªông)
    content_embeddings = model.encode(
        content_texts,
        show_progress_bar=True,
        batch_size=32,
        convert_to_numpy=True
    )
    
    print(f"\n‚úÖ Content embeddings created: {content_embeddings.shape}")
    print(f"   - Shape: ({len(movies)}, 384)")
    print(f"   - Sample embedding (first 5 values): {content_embeddings[0][:5]}")
    
    # L∆∞u model v√† embeddings
    model_path = os.path.join(MODELS_DIR, "sentence_bert_model")
    model.save(model_path)
    print(f"   - Model saved to: {model_path}")
    
    np.save(os.path.join(OUTPUT_DIR, "content_embeddings.npy"), content_embeddings)
    print(f"   - Embeddings saved to: {OUTPUT_DIR}/content_embeddings.npy")
    
    return content_embeddings, model

def create_collaborative_embeddings(ratings, n_factors=50):
    """
    T·∫°o Collaborative Embeddings b·∫±ng SVD (Matrix Factorization)
    H·ªçc t·ª´ rating patterns c·ªßa users
    """
    print("\n" + "=" * 80)
    print("CREATING COLLABORATIVE EMBEDDINGS (SVD)")
    print("=" * 80)
    
    print("\n1. Preparing data for SVD...")
    # Chu·∫©n b·ªã data cho Surprise
    reader = Reader(rating_scale=(0.5, 5.0))
    data = Dataset.load_from_df(
        ratings[['userId', 'movieId', 'rating']],
        reader
    )
    trainset = data.build_full_trainset()
    
    print(f"   - Users: {trainset.n_users}")
    print(f"   - Items (movies): {trainset.n_items}")
    print(f"   - Ratings: {trainset.n_ratings}")
    
    print(f"\n2. Training SVD model (n_factors={n_factors})...")
    print("   - This may take a few minutes...")
    
    # Train SVD
    algo = SVD(n_factors=n_factors, random_state=42, verbose=False)
    algo.fit(trainset)
    
    print("   ‚úÖ SVD model trained!")
    
    # L·∫•y item embeddings (qi trong SVD)
    # qi l√† item factors (embeddings cho m·ªói movie)
    print("\n3. Extracting item embeddings...")
    
    # C·∫ßn map movieId sang internal item id c·ªßa Surprise
    # T·∫°o mapping
    movie_id_to_inner_id = {trainset.to_raw_iid(i): i for i in range(trainset.n_items)}
    
    # L·∫•y embeddings cho t·∫•t c·∫£ items
    item_embeddings = np.array([algo.qi[i] for i in range(trainset.n_items)])
    
    print(f"   ‚úÖ Item embeddings extracted: {item_embeddings.shape}")
    print(f"   - Shape: ({trainset.n_items}, {n_factors})")
    print(f"   - Sample embedding (first 5 values): {item_embeddings[0][:5]}")
    
    # L∆∞u model v√† embeddings
    model_path = os.path.join(MODELS_DIR, "svd_model.pkl")
    with open(model_path, 'wb') as f:
        pickle.dump(algo, f)
    
    # L∆∞u mapping
    mapping_path = os.path.join(MODELS_DIR, "movie_id_mapping.pkl")
    with open(mapping_path, 'wb') as f:
        pickle.dump({
            'movie_id_to_inner_id': movie_id_to_inner_id,
            'inner_id_to_movie_id': {v: k for k, v in movie_id_to_inner_id.items()}
        }, f)
    
    print(f"   - Model saved to: {model_path}")
    print(f"   - Mapping saved to: {mapping_path}")
    
    np.save(os.path.join(OUTPUT_DIR, "collaborative_embeddings.npy"), item_embeddings)
    print(f"   - Embeddings saved to: {OUTPUT_DIR}/collaborative_embeddings.npy")
    
    return item_embeddings, algo, movie_id_to_inner_id

def create_hybrid_embeddings(content_emb, collab_emb, movies, movie_id_mapping):
    """
    K·∫øt h·ª£p Content v√† Collaborative Embeddings th√†nh Hybrid Embeddings
    """
    print("\n" + "=" * 80)
    print("CREATING HYBRID EMBEDDINGS")
    print("=" * 80)
    
    print("\n1. Aligning embeddings...")
    # Content embeddings c√≥ cho t·∫•t c·∫£ movies
    # Collaborative embeddings ch·ªâ c√≥ cho movies c√≥ ratings
    
    # T·∫°o matrix cho t·∫•t c·∫£ movies
    n_movies = len(movies)
    content_dim = content_emb.shape[1]  # 384
    collab_dim = collab_emb.shape[1]    # 50
    
    print(f"   - Total movies: {n_movies}")
    print(f"   - Content dim: {content_dim}")
    print(f"   - Collaborative dim: {collab_dim}")
    
    # T·∫°o full collaborative embeddings matrix
    # Movies kh√¥ng c√≥ rating ‚Üí zero vector
    full_collab_emb = np.zeros((n_movies, collab_dim))
    
    for idx, row in movies.iterrows():
        movie_id = row['movieId']
        if movie_id in movie_id_mapping:
            inner_id = movie_id_mapping[movie_id]
            if inner_id < len(collab_emb):
                full_collab_emb[idx] = collab_emb[inner_id]
    
    print(f"   - Movies with collaborative embeddings: {(full_collab_emb.sum(axis=1) != 0).sum()}")
    
    print("\n2. Normalizing embeddings...")
    # Normalize ƒë·ªÉ tr√°nh scale kh√°c nhau
    content_emb_norm = normalize(content_emb, norm='l2', axis=1)
    collab_emb_norm = normalize(full_collab_emb, norm='l2', axis=1)
    
    print("\n3. Creating hybrid embeddings (concatenate)...")
    # Option 1: Concatenate (gi·ªØ nguy√™n c·∫£ 2)
    hybrid_emb_concat = np.concatenate([content_emb_norm, collab_emb_norm], axis=1)
    
    print(f"   ‚úÖ Hybrid embeddings (concatenate): {hybrid_emb_concat.shape}")
    print(f"   - Shape: ({n_movies}, {content_dim + collab_dim}) = ({n_movies}, 434)")
    
    print("\n4. Creating hybrid embeddings (weighted average)...")
    # Option 2: Weighted average (gi·ªØ c√πng dimension)
    # Ch·ªâ d√πng khi c·∫£ 2 c√≥ c√πng dimension, ho·∫∑c project v·ªÅ c√πng dimension
    # ·ªû ƒë√¢y ta d√πng concatenate v√¨ dimensions kh√°c nhau
    
    # L∆∞u c·∫£ 2 lo·∫°i
    np.save(os.path.join(OUTPUT_DIR, "hybrid_embeddings_concat.npy"), hybrid_emb_concat)
    print(f"   - Saved: {OUTPUT_DIR}/hybrid_embeddings_concat.npy")
    
    # T·∫°o metadata
    metadata = {
        'content_dim': content_dim,
        'collab_dim': collab_dim,
        'hybrid_dim': content_dim + collab_dim,
        'n_movies': n_movies,
        'method': 'concatenate'
    }
    
    import json
    with open(os.path.join(OUTPUT_DIR, "embeddings_metadata.json"), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"   - Metadata saved: {OUTPUT_DIR}/embeddings_metadata.json")
    
    return hybrid_emb_concat, metadata

def create_movie_embedding_mapping(movies, hybrid_emb):
    """
    T·∫°o mapping movieId -> embedding index
    """
    print("\n" + "=" * 80)
    print("CREATING MOVIE EMBEDDING MAPPING")
    print("=" * 80)
    
    mapping = {}
    for idx, row in movies.iterrows():
        mapping[int(row['movieId'])] = idx
    
    mapping_path = os.path.join(OUTPUT_DIR, "movie_embedding_mapping.pkl")
    with open(mapping_path, 'wb') as f:
        pickle.dump(mapping, f)
    
    print(f"‚úÖ Mapping created: {len(mapping)} movies")
    print(f"   - Saved to: {mapping_path}")
    
    return mapping

def main():
    """H√†m ch√≠nh"""
    print("\n" + "=" * 80)
    print("CREATING HYBRID EMBEDDINGS (Content + Collaborative)")
    print("=" * 80)
    print("\n‚ö†Ô∏è  L∆ØU √ù: Qu√° tr√¨nh n√†y s·∫Ω:")
    print("   1. Download Sentence-BERT model (l·∫ßn ƒë·∫ßu ~90MB)")
    print("   2. Train SVD model (v√†i ph√∫t)")
    print("   3. T·∫°o embeddings cho t·∫•t c·∫£ movies")
    print("   ‚Üí T·ªïng th·ªùi gian: ~5-10 ph√∫t")
    print("=" * 80)
    
    # Load data
    movies, ratings = load_cleaned_data()
    
    # 1. Content embeddings
    content_emb, content_model = create_content_embeddings(movies)
    
    # 2. Collaborative embeddings
    collab_emb, svd_model, movie_id_mapping = create_collaborative_embeddings(ratings)
    
    # 3. Hybrid embeddings
    hybrid_emb, metadata = create_hybrid_embeddings(
        content_emb, collab_emb, movies, movie_id_mapping
    )
    
    # 4. Mapping
    embedding_mapping = create_movie_embedding_mapping(movies, hybrid_emb)
    
    # T√≥m t·∫Øt
    print("\n" + "=" * 80)
    print("T√ìM T·∫ÆT")
    print("=" * 80)
    print("\n‚úÖ ƒê√É T·∫†O:")
    print(f"   1. Content Embeddings: {content_emb.shape} (Sentence-BERT)")
    print(f"   2. Collaborative Embeddings: {collab_emb.shape} (SVD)")
    print(f"   3. Hybrid Embeddings: {hybrid_emb.shape} (Concatenate)")
    print(f"   4. Models v√† mappings ƒë√£ l∆∞u")
    
    print("\nüìÅ FILES CREATED:")
    print(f"   - {OUTPUT_DIR}/content_embeddings.npy")
    print(f"   - {OUTPUT_DIR}/collaborative_embeddings.npy")
    print(f"   - {OUTPUT_DIR}/hybrid_embeddings_concat.npy")
    print(f"   - {OUTPUT_DIR}/embeddings_metadata.json")
    print(f"   - {OUTPUT_DIR}/movie_embedding_mapping.pkl")
    print(f"   - {MODELS_DIR}/sentence_bert_model/")
    print(f"   - {MODELS_DIR}/svd_model.pkl")
    print(f"   - {MODELS_DIR}/movie_id_mapping.pkl")
    
    print("\nüìã C√ÅC B∆Ø·ªöC TI·∫æP THEO:")
    print("   1. S·ª≠ d·ª•ng hybrid_embeddings trong content_based.py")
    print("   2. Thay th·∫ø TF-IDF b·∫±ng hybrid embeddings")
    print("   3. Test v√† ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng")
    
    print("\n" + "=" * 80)
    print("HO√ÄN TH√ÄNH!")
    print("=" * 80)

if __name__ == "__main__":
    main()

