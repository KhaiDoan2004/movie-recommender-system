"""
Evaluation Module - ƒê√°nh gi√° Recommendation Models
Metrics: RMSE, MAE, Precision@K, Recall@K
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import os
import warnings
warnings.filterwarnings('ignore')

# ƒê∆∞·ªùng d·∫´n - relative t·ª´ project root
BASE_DIR = Path(__file__).parent.parent.parent
DATA_DIR = str(BASE_DIR / "data_cleaned")

class RecommendationEvaluator:
    """Class ƒë·ªÉ ƒë√°nh gi√° recommendation models"""
    
    def __init__(self, test_size=0.2, random_state=42):
        """
        Kh·ªüi t·∫°o evaluator
        
        Args:
            test_size: T·ª∑ l·ªá test set
            random_state: Random seed
        """
        print("=" * 80)
        print("INITIALIZING EVALUATOR")
        print("=" * 80)
        
        # Load ratings
        self.ratings = pd.read_csv(os.path.join(DATA_DIR, "ratings_cleaned.csv"))
        print(f"‚úÖ Loaded {len(self.ratings)} ratings")
        
        # Split train/test
        self.train_ratings, self.test_ratings = train_test_split(
            self.ratings,
            test_size=test_size,
            random_state=random_state,
            stratify=None
        )
        print(f"‚úÖ Train set: {len(self.train_ratings)} ratings")
        print(f"‚úÖ Test set: {len(self.test_ratings)} ratings")
        
        # Load models
        self._load_models()
    
    def _load_models(self):
        """Load recommendation models"""
        print("\nüì¶ Loading recommendation models...")
        
        from recommender.models import ContentBasedRecommender, CollaborativeRecommender, HybridRecommender
        
        print("   ‚Üí Loading Content-Based...")
        self.content_recommender = ContentBasedRecommender(use_hybrid=True)
        
        print("   ‚Üí Loading Collaborative...")
        # C·∫ßn train l·∫°i SVD tr√™n train set (s·∫Ω l√†m sau)
        self.collab_recommender = None  # S·∫Ω train l·∫°i
        
        print("   ‚Üí Loading Hybrid...")
        self.hybrid_recommender = HybridRecommender()
        
        print("   ‚úÖ Models loaded")
    
    def calculate_rmse_mae(self, y_true, y_pred):
        """
        T√≠nh RMSE v√† MAE
        
        Args:
            y_true: True ratings
            y_pred: Predicted ratings
        
        Returns:
            rmse, mae
        """
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        return rmse, mae
    
    def evaluate_collaborative_rmse_mae(self):
        """
        ƒê√°nh gi√° Collaborative Filtering v·ªõi RMSE v√† MAE
        Train SVD tr√™n train set, test tr√™n test set
        """
        print("\n" + "=" * 80)
        print("EVALUATING COLLABORATIVE FILTERING (RMSE, MAE)")
        print("=" * 80)
        
        from surprise import SVD, Dataset, Reader
        
        # Train SVD tr√™n train set
        print("\nüîÑ Training SVD on train set...")
        reader = Reader(rating_scale=(0.5, 5.0))
        train_data = Dataset.load_from_df(
            self.train_ratings[['userId', 'movieId', 'rating']],
            reader
        )
        trainset = train_data.build_full_trainset()
        
        svd = SVD(n_factors=50, random_state=42, verbose=False)
        svd.fit(trainset)
        print("   ‚úÖ SVD trained")
        
        # Predict tr√™n test set
        print("\nüîÑ Predicting on test set...")
        predictions = []
        actuals = []
        
        for idx, row in self.test_ratings.iterrows():
            try:
                pred = svd.predict(row['userId'], row['movieId'])
                predictions.append(pred.est)
                actuals.append(row['rating'])
            except:
                # N·∫øu kh√¥ng predict ƒë∆∞·ª£c, d√πng average rating
                predictions.append(self.train_ratings['rating'].mean())
                actuals.append(row['rating'])
        
        # T√≠nh metrics
        rmse, mae = self.calculate_rmse_mae(actuals, predictions)
        
        print(f"\nüìä Results:")
        print(f"   RMSE: {rmse:.4f}")
        print(f"   MAE:  {mae:.4f}")
        
        return {'rmse': rmse, 'mae': mae, 'predictions': predictions, 'actuals': actuals}
    
    def precision_at_k(self, recommended, relevant, k):
        """
        T√≠nh Precision@K
        
        Args:
            recommended: List of recommended movie IDs
            relevant: Set of relevant movie IDs (user ƒë√£ rate cao)
            k: Top K recommendations
        
        Returns:
            Precision@K
        """
        recommended_k = recommended[:k]
        if len(recommended_k) == 0:
            return 0.0
        
        relevant_recommended = len(set(recommended_k) & set(relevant))
        return relevant_recommended / len(recommended_k)
    
    def recall_at_k(self, recommended, relevant, k):
        """
        T√≠nh Recall@K
        
        Args:
            recommended: List of recommended movie IDs
            relevant: Set of relevant movie IDs
            k: Top K recommendations
        
        Returns:
            Recall@K
        """
        recommended_k = recommended[:k]
        if len(relevant) == 0:
            return 0.0
        
        relevant_recommended = len(set(recommended_k) & set(relevant))
        return relevant_recommended / len(relevant)
    
    def evaluate_precision_recall_at_k(self, recommender, recommender_name, k=10, threshold=4.0, n_users=50):
        """
        ƒê√°nh gi√° Precision@K v√† Recall@K
        
        Args:
            recommender: Recommender object
            recommender_name: T√™n recommender
            k: Top K recommendations
            threshold: Rating threshold ƒë·ªÉ coi l√† relevant (>= threshold)
            n_users: S·ªë users ƒë·ªÉ test (ƒë·ªÉ nhanh h∆°n)
        
        Returns:
            dict v·ªõi precision@k v√† recall@k
        """
        print(f"\n" + "=" * 80)
        print(f"EVALUATING {recommender_name.upper()} (Precision@{k}, Recall@{k})")
        print("=" * 80)
        
        # L·∫•y sample users t·ª´ test set
        test_users = self.test_ratings['userId'].unique()[:n_users]
        print(f"\nüìä Testing on {len(test_users)} users...")
        
        precisions = []
        recalls = []
        
        for user_id in test_users:
            # L·∫•y ratings c·ªßa user trong train set (ƒë·ªÉ l√†m history)
            user_train_ratings = self.train_ratings[
                self.train_ratings['userId'] == user_id
            ]
            
            # L·∫•y ratings c·ªßa user trong test set (ƒë·ªÉ l√†m ground truth)
            user_test_ratings = self.test_ratings[
                self.test_ratings['userId'] == user_id
            ]
            
            # Relevant movies: phim user rate >= threshold trong test set
            relevant_movies = set(
                user_test_ratings[user_test_ratings['rating'] >= threshold]['movieId'].tolist()
            )
            
            if len(relevant_movies) == 0:
                continue  # Skip n·∫øu kh√¥ng c√≥ relevant movies
            
            # T·∫°o user history t·ª´ train set
            user_history = dict(zip(
                user_train_ratings['movieId'],
                user_train_ratings['rating']
            ))
            
            # Get recommendations
            try:
                if recommender_name == "Content-Based":
                    recommendations_df = recommender.recommend_for_user_content_only(
                        user_history, n=k
                    )
                elif recommender_name == "Hybrid":
                    recommendations_df = recommender.recommend(user_history, n=k)
                elif recommender_name == "Collaborative":
                    recommendations_df = recommender.recommend_for_user(user_history, n=k)
                else:
                    recommendations_df = recommender.recommend_for_user(user_history, n=k)
                
                if len(recommendations_df) > 0:
                    recommended_movies = recommendations_df['movieId'].tolist()
                    
                    # T√≠nh precision v√† recall
                    precision = self.precision_at_k(recommended_movies, relevant_movies, k)
                    recall = self.recall_at_k(recommended_movies, relevant_movies, k)
                    
                    precisions.append(precision)
                    recalls.append(recall)
            except Exception as e:
                # Skip n·∫øu c√≥ l·ªói
                continue
        
        # T√≠nh average
        avg_precision = np.mean(precisions) if len(precisions) > 0 else 0.0
        avg_recall = np.mean(recalls) if len(recalls) > 0 else 0.0
        
        print(f"\nüìä Results:")
        print(f"   Precision@{k}: {avg_precision:.4f}")
        print(f"   Recall@{k}:    {avg_recall:.4f}")
        print(f"   Users evaluated: {len(precisions)}")
        
        return {
            'precision@k': avg_precision,
            'recall@k': avg_recall,
            'n_users': len(precisions)
        }
    
    def evaluate_all(self, k=10):
        """
        ƒê√°nh gi√° t·∫•t c·∫£ models
        
        Args:
            k: Top K cho Precision@K v√† Recall@K
        
        Returns:
            dict v·ªõi t·∫•t c·∫£ k·∫øt qu·∫£
        """
        print("\n" + "=" * 80)
        print("EVALUATING ALL MODELS")
        print("=" * 80)
        
        results = {}
        
        # 1. Collaborative: RMSE, MAE
        print("\n" + "-" * 80)
        collab_results = self.evaluate_collaborative_rmse_mae()
        results['collaborative_rmse_mae'] = collab_results
        
        # 2. Content-Based: Precision@K, Recall@K
        print("\n" + "-" * 80)
        content_results = self.evaluate_precision_recall_at_k(
            self.content_recommender,
            "Content-Based",
            k=k
        )
        results['content_based'] = content_results
        
        # 3. Collaborative: Precision@K, Recall@K
        print("\n" + "-" * 80)
        # D√πng collaborative recommender t·ª´ hybrid (ƒë√£ load s·∫µn)
        collab_results_pr = self.evaluate_precision_recall_at_k(
            self.hybrid_recommender.collab_recommender,
            "Collaborative",
            k=k
        )
        results['collaborative_pr'] = collab_results_pr
        
        # 4. Hybrid: Precision@K, Recall@K
        print("\n" + "-" * 80)
        hybrid_results = self.evaluate_precision_recall_at_k(
            self.hybrid_recommender,
            "Hybrid",
            k=k
        )
        results['hybrid'] = hybrid_results
        
        # T·ªïng k·∫øt
        print("\n" + "=" * 80)
        print("SUMMARY")
        print("=" * 80)
        print(f"\nüìä Collaborative Filtering (Rating Prediction):")
        print(f"   RMSE: {results['collaborative_rmse_mae']['rmse']:.4f}")
        print(f"   MAE:  {results['collaborative_rmse_mae']['mae']:.4f}")
        
        print(f"\nüìä Content-Based (Top-{k} Recommendations):")
        print(f"   Precision@{k}: {results['content_based']['precision@k']:.4f}")
        print(f"   Recall@{k}:    {results['content_based']['recall@k']:.4f}")
        
        print(f"\nüìä Collaborative (Top-{k} Recommendations):")
        print(f"   Precision@{k}: {results['collaborative_pr']['precision@k']:.4f}")
        print(f"   Recall@{k}:    {results['collaborative_pr']['recall@k']:.4f}")
        
        print(f"\nüìä Hybrid (Top-{k} Recommendations):")
        print(f"   Precision@{k}: {results['hybrid']['precision@k']:.4f}")
        print(f"   Recall@{k}:    {results['hybrid']['recall@k']:.4f}")
        
        return results

def main():
    """Ch·∫°y evaluation"""
    print("\n" + "=" * 80)
    print("RECOMMENDATION MODELS EVALUATION")
    print("=" * 80)
    print("\n‚ö†Ô∏è  L∆∞u √Ω: Qu√° tr√¨nh n√†y s·∫Ω m·∫•t v√†i ph√∫t ƒë·ªÉ:")
    print("   1. Load models")
    print("   2. Train SVD tr√™n train set")
    print("   3. Evaluate tr√™n test set")
    print("=" * 80)
    
    # Kh·ªüi t·∫°o evaluator
    evaluator = RecommendationEvaluator(test_size=0.2, random_state=42)
    
    # ƒê√°nh gi√° t·∫•t c·∫£
    results = evaluator.evaluate_all(k=10)
    
    print("\n" + "=" * 80)
    print("‚úÖ EVALUATION COMPLETE!")
    print("=" * 80)
    
    return results

if __name__ == "__main__":
    results = main()

