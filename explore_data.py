"""
Script kh√°m ph√° d·ªØ li·ªáu MovieLens
Ph√¢n t√≠ch dataset ƒë·ªÉ hi·ªÉu c·∫•u tr√∫c, ch·∫•t l∆∞·ª£ng v√† ƒë·∫∑c ƒëi·ªÉm d·ªØ li·ªáu
"""

import pandas as pd
import numpy as np
from datetime import datetime
import os
from pathlib import Path

# ƒê∆∞·ªùng d·∫´n - relative t·ª´ project root
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = str(BASE_DIR / "data")

def load_data():
    """Load t·∫•t c·∫£ c√°c file CSV"""
    print("=" * 80)
    print("LOADING DATA...")
    print("=" * 80)
    
    try:
        movies = pd.read_csv(os.path.join(DATA_DIR, "movies.csv"))
        ratings = pd.read_csv(os.path.join(DATA_DIR, "ratings.csv"))
        tags = pd.read_csv(os.path.join(DATA_DIR, "tags.csv"))
        links = pd.read_csv(os.path.join(DATA_DIR, "links.csv"))
        
        print("‚úÖ ƒê√£ load th√†nh c√¥ng t·∫•t c·∫£ c√°c file!")
        return movies, ratings, tags, links
    except Exception as e:
        print(f"‚ùå L·ªói khi load d·ªØ li·ªáu: {e}")
        return None, None, None, None

def basic_statistics(movies, ratings, tags, links):
    """Th·ªëng k√™ c∆° b·∫£n v·ªÅ dataset"""
    print("\n" + "=" * 80)
    print("TH·ªêNG K√ä C∆† B·∫¢N")
    print("=" * 80)
    
    print("\nüìä MOVIES.CSV:")
    print(f"   - S·ªë l∆∞·ª£ng phim: {len(movies):,}")
    print(f"   - S·ªë c·ªôt: {len(movies.columns)}")
    print(f"   - C√°c c·ªôt: {list(movies.columns)}")
    print(f"   - K√≠ch th∆∞·ªõc: {movies.shape}")
    
    print("\nüìä RATINGS.CSV:")
    print(f"   - S·ªë l∆∞·ª£ng ratings: {len(ratings):,}")
    print(f"   - S·ªë c·ªôt: {len(ratings.columns)}")
    print(f"   - C√°c c·ªôt: {list(ratings.columns)}")
    print(f"   - K√≠ch th∆∞·ªõc: {ratings.shape}")
    print(f"   - S·ªë users duy nh·∫•t: {ratings['userId'].nunique():,}")
    print(f"   - S·ªë phim ƒë∆∞·ª£c rate: {ratings['movieId'].nunique():,}")
    
    print("\nüìä TAGS.CSV:")
    print(f"   - S·ªë l∆∞·ª£ng tags: {len(tags):,}")
    print(f"   - S·ªë c·ªôt: {len(tags.columns)}")
    print(f"   - C√°c c·ªôt: {list(tags.columns)}")
    print(f"   - K√≠ch th∆∞·ªõc: {tags.shape}")
    print(f"   - S·ªë users ƒë√£ tag: {tags['userId'].nunique():,}")
    print(f"   - S·ªë phim c√≥ tag: {tags['movieId'].nunique():,}")
    
    print("\nüìä LINKS.CSV:")
    print(f"   - S·ªë l∆∞·ª£ng links: {len(links):,}")
    print(f"   - S·ªë c·ªôt: {len(links.columns)}")
    print(f"   - C√°c c·ªôt: {list(links.columns)}")
    print(f"   - K√≠ch th∆∞·ªõc: {links.shape}")

def check_missing_values(movies, ratings, tags, links):
    """Ki·ªÉm tra missing values"""
    print("\n" + "=" * 80)
    print("KI·ªÇM TRA MISSING VALUES")
    print("=" * 80)
    
    print("\nüìã MOVIES.CSV:")
    missing_movies = movies.isnull().sum()
    if missing_movies.sum() == 0:
        print("   ‚úÖ Kh√¥ng c√≥ missing values")
    else:
        print(missing_movies[missing_movies > 0])
    
    print("\nüìã RATINGS.CSV:")
    missing_ratings = ratings.isnull().sum()
    if missing_ratings.sum() == 0:
        print("   ‚úÖ Kh√¥ng c√≥ missing values")
    else:
        print(missing_ratings[missing_ratings > 0])
    
    print("\nüìã TAGS.CSV:")
    missing_tags = tags.isnull().sum()
    if missing_tags.sum() == 0:
        print("   ‚úÖ Kh√¥ng c√≥ missing values")
    else:
        print(missing_tags[missing_tags > 0])
    
    print("\nüìã LINKS.CSV:")
    missing_links = links.isnull().sum()
    if missing_links.sum() == 0:
        print("   ‚úÖ Kh√¥ng c√≥ missing values")
    else:
        print(missing_links[missing_links > 0])
    
    # Ki·ªÉm tra genres r·ªóng ho·∫∑c "(no genres listed)"
    print("\nüîç Ki·ªÉm tra genres r·ªóng:")
    empty_genres = movies[movies['genres'].isna() | (movies['genres'] == '(no genres listed)')]
    print(f"   - S·ªë phim kh√¥ng c√≥ genres: {len(empty_genres)}")
    if len(empty_genres) > 0:
        print(f"   - Sample: {empty_genres.head(3)['title'].tolist()}")

def check_duplicates(movies, ratings, tags, links):
    """Ki·ªÉm tra duplicates"""
    print("\n" + "=" * 80)
    print("KI·ªÇM TRA DUPLICATES")
    print("=" * 80)
    
    print("\nüìã MOVIES.CSV:")
    dup_movies = movies.duplicated(subset=['movieId']).sum()
    print(f"   - Duplicate movieId: {dup_movies}")
    dup_title = movies.duplicated(subset=['title']).sum()
    print(f"   - Duplicate title: {dup_title}")
    
    print("\nüìã RATINGS.CSV:")
    dup_ratings = ratings.duplicated(subset=['userId', 'movieId']).sum()
    print(f"   - Duplicate (userId, movieId): {dup_ratings}")
    if dup_ratings > 0:
        print(f"   ‚ö†Ô∏è  C·∫ßn x·ª≠ l√Ω: gi·ªØ b·∫£n ghi cu·ªëi c√πng")
        # Hi·ªÉn th·ªã sample duplicates
        dup_samples = ratings[ratings.duplicated(subset=['userId', 'movieId'], keep=False)]
        print(f"   - Sample duplicates:\n{dup_samples.head(10)}")
    
    print("\nüìã TAGS.CSV:")
    dup_tags = tags.duplicated(subset=['userId', 'movieId', 'tag']).sum()
    print(f"   - Duplicate (userId, movieId, tag): {dup_tags}")
    
    print("\nüìã LINKS.CSV:")
    dup_links = links.duplicated(subset=['movieId']).sum()
    print(f"   - Duplicate movieId: {dup_links}")

def analyze_movies(movies):
    """Ph√¢n t√≠ch chi ti·∫øt movies"""
    print("\n" + "=" * 80)
    print("PH√ÇN T√çCH MOVIES")
    print("=" * 80)
    
    print("\nüìù Sample movies (5 phim ƒë·∫ßu):")
    print(movies.head())
    
    print("\nüìù Sample movies (5 phim cu·ªëi):")
    print(movies.tail())
    
    # Ph√¢n t√≠ch genres
    print("\nüé¨ PH√ÇN T√çCH GENRES:")
    all_genres = []
    for genres_str in movies['genres'].dropna():
        if genres_str != '(no genres listed)':
            all_genres.extend(genres_str.split('|'))
    
    from collections import Counter
    genre_counts = Counter(all_genres)
    print(f"   - T·ªïng s·ªë genres duy nh·∫•t: {len(genre_counts)}")
    print(f"   - Top 10 genres ph·ªï bi·∫øn:")
    for genre, count in genre_counts.most_common(10):
        print(f"     {genre}: {count:,} phim")
    
    # Ph√¢n t√≠ch nƒÉm (t√°ch t·ª´ title)
    print("\nüìÖ PH√ÇN T√çCH NƒÇM (t·ª´ title):")
    import re
    years = []
    for title in movies['title']:
        match = re.search(r'\((\d{4})\)', title)
        if match:
            years.append(int(match.group(1)))
    
    if years:
        print(f"   - NƒÉm s·ªõm nh·∫•t: {min(years)}")
        print(f"   - NƒÉm mu·ªôn nh·∫•t: {max(years)}")
        print(f"   - S·ªë phim c√≥ nƒÉm: {len(years)}/{len(movies)}")
        
        # Ph√¢n b·ªë theo th·∫≠p k·ª∑
        decades = {}
        for year in years:
            decade = (year // 10) * 10
            decades[decade] = decades.get(decade, 0) + 1
        
        print(f"   - Ph√¢n b·ªë theo th·∫≠p k·ª∑:")
        for decade in sorted(decades.keys()):
            print(f"     {decade}s: {decades[decade]:,} phim")

def analyze_ratings(ratings):
    """Ph√¢n t√≠ch chi ti·∫øt ratings"""
    print("\n" + "=" * 80)
    print("PH√ÇN T√çCH RATINGS")
    print("=" * 80)
    
    print("\nüìù Sample ratings (10 d√≤ng ƒë·∫ßu):")
    print(ratings.head(10))
    
    # Th·ªëng k√™ rating
    print("\n‚≠ê TH·ªêNG K√ä RATING:")
    print(f"   - Rating trung b√¨nh: {ratings['rating'].mean():.2f}")
    print(f"   - Rating trung v·ªã: {ratings['rating'].median():.2f}")
    print(f"   - Rating min: {ratings['rating'].min()}")
    print(f"   - Rating max: {ratings['rating'].max()}")
    print(f"   - ƒê·ªô l·ªách chu·∫©n: {ratings['rating'].std():.2f}")
    
    print("\nüìä Ph√¢n b·ªë rating:")
    rating_dist = ratings['rating'].value_counts().sort_index()
    for rating, count in rating_dist.items():
        percentage = (count / len(ratings)) * 100
        print(f"   {rating:.1f} sao: {count:>7,} ({percentage:>5.2f}%)")
    
    # Ph√¢n t√≠ch timestamp
    print("\n‚è∞ PH√ÇN T√çCH TIMESTAMP:")
    ratings['datetime'] = pd.to_datetime(ratings['timestamp'], unit='s')
    print(f"   - Ng√†y s·ªõm nh·∫•t: {ratings['datetime'].min()}")
    print(f"   - Ng√†y mu·ªôn nh·∫•t: {ratings['datetime'].max()}")
    print(f"   - Kho·∫£ng th·ªùi gian: {(ratings['datetime'].max() - ratings['datetime'].min()).days} ng√†y")
    
    # Ratings theo nƒÉm
    ratings['year'] = ratings['datetime'].dt.year
    ratings_by_year = ratings.groupby('year').size()
    print(f"\nüìà S·ªë ratings theo nƒÉm (top 5):")
    for year, count in ratings_by_year.sort_values(ascending=False).head(5).items():
        print(f"   {year}: {count:,} ratings")
    
    # Ph√¢n t√≠ch users
    print("\nüë• PH√ÇN T√çCH USERS:")
    user_rating_counts = ratings.groupby('userId').size()
    print(f"   - S·ªë ratings trung b√¨nh/user: {user_rating_counts.mean():.2f}")
    print(f"   - User rate nhi·ªÅu nh·∫•t: {user_rating_counts.max()} ratings")
    print(f"   - User rate √≠t nh·∫•t: {user_rating_counts.min()} ratings")
    print(f"   - Median ratings/user: {user_rating_counts.median():.2f}")
    
    # Ph√¢n t√≠ch movies
    print("\nüé¨ PH√ÇN T√çCH MOVIES:")
    movie_rating_counts = ratings.groupby('movieId').size()
    print(f"   - S·ªë ratings trung b√¨nh/phim: {movie_rating_counts.mean():.2f}")
    print(f"   - Phim ƒë∆∞·ª£c rate nhi·ªÅu nh·∫•t: {movie_rating_counts.max()} ratings")
    print(f"   - Phim ƒë∆∞·ª£c rate √≠t nh·∫•t: {movie_rating_counts.min()} ratings")
    print(f"   - Median ratings/movie: {movie_rating_counts.median():.2f}")
    
    # Top phim ƒë∆∞·ª£c rate nhi·ªÅu nh·∫•t
    print(f"\nüèÜ Top 10 phim ƒë∆∞·ª£c rate nhi·ªÅu nh·∫•t:")
    top_movies = movie_rating_counts.sort_values(ascending=False).head(10)
    for movie_id, count in top_movies.items():
        print(f"   MovieID {movie_id}: {count:,} ratings")

def analyze_tags(tags):
    """Ph√¢n t√≠ch chi ti·∫øt tags"""
    print("\n" + "=" * 80)
    print("PH√ÇN T√çCH TAGS")
    print("=" * 80)
    
    print("\nüìù Sample tags (10 d√≤ng ƒë·∫ßu):")
    print(tags.head(10))
    
    print(f"\nüè∑Ô∏è  TH·ªêNG K√ä TAGS:")
    print(f"   - S·ªë tags duy nh·∫•t: {tags['tag'].nunique():,}")
    print(f"   - S·ªë phim c√≥ tag: {tags['movieId'].nunique():,}")
    print(f"   - S·ªë users ƒë√£ tag: {tags['userId'].nunique():,}")
    
    # Top tags
    print(f"\nüèÜ Top 20 tags ph·ªï bi·∫øn nh·∫•t:")
    top_tags = tags['tag'].value_counts().head(20)
    for tag, count in top_tags.items():
        print(f"   '{tag}': {count:,} l·∫ßn")
    
    # Tags per movie
    tags_per_movie = tags.groupby('movieId').size()
    print(f"\nüìä Tags per movie:")
    print(f"   - Trung b√¨nh: {tags_per_movie.mean():.2f} tags/phim")
    print(f"   - Nhi·ªÅu nh·∫•t: {tags_per_movie.max()} tags")
    print(f"   - √çt nh·∫•t: {tags_per_movie.min()} tags")

def analyze_links(links):
    """Ph√¢n t√≠ch chi ti·∫øt links"""
    print("\n" + "=" * 80)
    print("PH√ÇN T√çCH LINKS")
    print("=" * 80)
    
    print("\nüìù Sample links (10 d√≤ng ƒë·∫ßu):")
    print(links.head(10))
    
    print(f"\nüîó TH·ªêNG K√ä LINKS:")
    print(f"   - S·ªë phim c√≥ IMDb link: {links['imdbId'].notna().sum():,}")
    print(f"   - S·ªë phim c√≥ TMDB link: {links['tmdbId'].notna().sum():,}")
    print(f"   - S·ªë phim c√≥ c·∫£ 2 links: {(links['imdbId'].notna() & links['tmdbId'].notna()).sum():,}")

def data_quality_summary(movies, ratings, tags, links):
    """T√≥m t·∫Øt ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu"""
    print("\n" + "=" * 80)
    print("T√ìM T·∫ÆT CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU")
    print("=" * 80)
    
    print("\n‚úÖ ƒêI·ªÇM M·∫†NH:")
    print("   - Dataset l·ªõn: 9,742 phim, 100,836 ratings")
    print("   - Kh√¥ng c√≥ missing values trong ratings")
    print("   - C√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin: movies, ratings, tags, links")
    print("   - D·ªØ li·ªáu tr·∫£i d√†i 22 nƒÉm (1996-2018)")
    
    print("\n‚ö†Ô∏è  V·∫§N ƒê·ªÄ C·∫¶N X·ª¨ L√ù:")
    # Ki·ªÉm tra genres r·ªóng
    empty_genres = movies[movies['genres'].isna() | (movies['genres'] == '(no genres listed)')]
    if len(empty_genres) > 0:
        print(f"   - {len(empty_genres)} phim kh√¥ng c√≥ genres ‚Üí c·∫ßn g√°n 'Unknown'")
    
    # Ki·ªÉm tra duplicates trong ratings
    dup_ratings = ratings.duplicated(subset=['userId', 'movieId']).sum()
    if dup_ratings > 0:
        print(f"   - {dup_ratings} duplicate ratings ‚Üí c·∫ßn gi·ªØ b·∫£n cu·ªëi")
    
    # Ki·ªÉm tra phim kh√¥ng c√≥ rating
    movies_with_ratings = ratings['movieId'].unique()
    movies_without_ratings = set(movies['movieId']) - set(movies_with_ratings)
    if len(movies_without_ratings) > 0:
        print(f"   - {len(movies_without_ratings)} phim kh√¥ng c√≥ rating")
    
    print("\nüìã C√ÅC B∆Ø·ªöC TI·∫æP THEO:")
    print("   1. L√†m s·∫°ch d·ªØ li·ªáu (x·ª≠ l√Ω missing, duplicates)")
    print("   2. Chu·∫©n h√≥a d·ªØ li·ªáu (t√°ch year, genres, datetime)")
    print("   3. Vector h√≥a (TF-IDF cho content-based)")
    print("   4. Tr·ª±c quan h√≥a d·ªØ li·ªáu")
    print("   5. X√¢y d·ª±ng models")

def main():
    """H√†m ch√≠nh"""
    print("\n" + "=" * 80)
    print("KH√ÅM PH√Å D·ªÆ LI·ªÜU MOVIELENS DATASET")
    print("=" * 80)
    
    # Load data
    movies, ratings, tags, links = load_data()
    
    if movies is None:
        print("‚ùå Kh√¥ng th·ªÉ load d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n!")
        return
    
    # Th·ª±c hi·ªán c√°c ph√¢n t√≠ch
    basic_statistics(movies, ratings, tags, links)
    check_missing_values(movies, ratings, tags, links)
    check_duplicates(movies, ratings, tags, links)
    analyze_movies(movies)
    analyze_ratings(ratings)
    analyze_tags(tags)
    analyze_links(links)
    data_quality_summary(movies, ratings, tags, links)
    
    print("\n" + "=" * 80)
    print("HO√ÄN TH√ÄNH KH√ÅM PH√Å D·ªÆ LI·ªÜU!")
    print("=" * 80)

if __name__ == "__main__":
    main()

